{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load a dataset from a file.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the dataset file.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The loaded dataset, or None if loading failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Strip whitespace and convert to lowercase for more reliable checking\n",
    "        clean_path = file_path.strip().lower()\n",
    "        \n",
    "        if clean_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif clean_path.endswith(('.xls', '.xlsx')):\n",
    "            df = pd.read_excel(file_path)\n",
    "        elif clean_path.endswith('.txt'):\n",
    "            # Try to load as a CSV with different delimiters\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=',')\n",
    "            except:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, sep='\\t')\n",
    "                except:\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, sep=';')\n",
    "                    except:\n",
    "                        print(f\"Could not determine delimiter for text file. Please specify the format manually.\")\n",
    "                        return None\n",
    "        else:\n",
    "            print(f\"Unsupported file format. Please provide a CSV, Excel, or TXT file.\")\n",
    "            print(f\"If your file is in a supported format but has a different extension, please rename it.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_data_quality(df):\n",
    "    \"\"\"\n",
    "    Check the quality of a dataset, identifying duplicates, missing values, and potential outliers.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset to check.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A report of data quality issues.\n",
    "    \"\"\"\n",
    "    quality_report = {\n",
    "        'duplicates': {},\n",
    "        'missing_values': {'count': {}, 'percentage': {}},\n",
    "        'potential_outliers': {}\n",
    "    }\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated()\n",
    "    quality_report['duplicates']['count'] = duplicates.sum()\n",
    "    quality_report['duplicates']['percentage'] = (duplicates.sum() / len(df)) * 100\n",
    "    \n",
    "    # Check for missing values\n",
    "    for col in df.columns:\n",
    "        missing = df[col].isnull()\n",
    "        quality_report['missing_values']['count'][col] = missing.sum()\n",
    "        quality_report['missing_values']['percentage'][col] = (missing.sum() / len(df)) * 100\n",
    "    \n",
    "    # Check for potential outliers (using IQR method) for numeric columns\n",
    "    for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        # Skip columns with too many unique values (likely IDs or continuous values)\n",
    "        if df[col].nunique() > 100:\n",
    "            continue\n",
    "            \n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "        outlier_count = outliers.sum()\n",
    "        outlier_percentage = (outlier_count / len(df)) * 100\n",
    "        \n",
    "        if outlier_count > 0:\n",
    "            quality_report['potential_outliers'][col] = {\n",
    "                'count': outlier_count,\n",
    "                'percentage': outlier_percentage,\n",
    "                'lower_bound': lower_bound,\n",
    "                'upper_bound': upper_bound\n",
    "            }\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "def visualize_distributions(df, n_cols=3):\n",
    "    \"\"\"\n",
    "    Create histograms for numerical features and bar plots for categorical features.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset to visualize.\n",
    "    n_cols (int): Number of columns in the subplot grid.\n",
    "    \"\"\"\n",
    "    # Separate numerical and categorical columns\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    # Visualize numerical columns\n",
    "    if len(num_cols) > 0:\n",
    "        n_rows = (len(num_cols) + n_cols - 1) // n_cols\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))\n",
    "        axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "        \n",
    "        for i, col in enumerate(num_cols):\n",
    "            if i < len(axes):\n",
    "                # TODO #1: Modify the histogram visualization below\n",
    "                # Instead of using sns.histplot, use plt.hist() to create a histogram\n",
    "                # Make sure to include df[col].dropna() as the data to plot\n",
    "                # Hint: plt.hist(df[col].dropna(), bins=10)\n",
    "                sns.histplot(df[col].dropna(), kde=True, ax=axes[i])\n",
    "                \n",
    "                # TODO #2: Add a vertical line at the mean value for this column\n",
    "                # Use axes[i].axvline() to add a vertical line\n",
    "                # Calculate the mean using df[col].mean()\n",
    "                # Make the line red and dashed\n",
    "                # Hint: axes[i].axvline(df[col].mean(), color='red', linestyle='--', label='Mean')\n",
    "                \n",
    "                # Set titles and labels\n",
    "                axes[i].set_title(f'Distribution of {col}')\n",
    "                axes[i].set_xlabel(col)\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "                \n",
    "                # TODO #3: Add a legend to show what the vertical line represents\n",
    "                # Hint: axes[i].legend()\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Visualize categorical columns\n",
    "    if len(cat_cols) > 0:\n",
    "        for col in cat_cols:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            value_counts = df[col].value_counts().sort_values(ascending=False)\n",
    "            if len(value_counts) > 15:  # If too many categories, show only top 15\n",
    "                value_counts = value_counts.iloc[:15]\n",
    "                plt.title(f'Top 15 values in {col}')\n",
    "            else:\n",
    "                plt.title(f'Distribution of {col}')\n",
    "            \n",
    "            # TODO #4: Enhance the categorical bar plot\n",
    "            # Instead of using sns.barplot, use plt.bar to create a bar chart\n",
    "            # You'll need to provide the x positions and the height (values)\n",
    "            # Also add a color parameter to make the bars blue\n",
    "            # Hint: plt.bar(range(len(value_counts)), value_counts.values, color='blue')\n",
    "            # Then add proper x-tick labels: plt.xticks(range(len(value_counts)), value_counts.index, rotation=45, ha='right')\n",
    "            sns.barplot(x=value_counts.index, y=value_counts.values)\n",
    "            \n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Count')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "def clean_dataset(df, quality_report=None):\n",
    "    \"\"\"\n",
    "    Clean the dataset based on identified issues.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset to clean.\n",
    "    quality_report (dict, optional): Data quality report generated by check_data_quality.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The cleaned dataset.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Generate quality report if not provided\n",
    "    if quality_report is None:\n",
    "        quality_report = check_data_quality(df)\n",
    "    \n",
    "    print(\"Starting data cleaning process...\")\n",
    "    \n",
    "    # Handle duplicates\n",
    "    initial_rows = len(cleaned_df)\n",
    "    cleaned_df = cleaned_df.drop_duplicates()\n",
    "    rows_removed = initial_rows - len(cleaned_df)\n",
    "    print(f\"Removed {rows_removed} duplicate rows.\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in cleaned_df.columns:\n",
    "        missing_pct = quality_report['missing_values']['percentage'][col]\n",
    "        \n",
    "        # If missing percentage is high (e.g., > 50%), consider dropping the column\n",
    "        if missing_pct > 50:\n",
    "            print(f\"Column '{col}' has {missing_pct:.2f}% missing values. Consider dropping this column.\")\n",
    "        \n",
    "        # For numeric columns, impute with median\n",
    "        elif col in cleaned_df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "            if missing_pct > 0:\n",
    "                median_val = cleaned_df[col].median()\n",
    "                cleaned_df[col].fillna(median_val, inplace=True)\n",
    "                print(f\"Imputed {missing_pct:.2f}% missing values in '{col}' with median: {median_val}\")\n",
    "        \n",
    "        # For categorical columns, impute with mode\n",
    "        elif col in cleaned_df.select_dtypes(include=['object', 'category']).columns:\n",
    "            if missing_pct > 0:\n",
    "                mode_val = cleaned_df[col].mode()[0]\n",
    "                cleaned_df[col].fillna(mode_val, inplace=True)\n",
    "                print(f\"Imputed {missing_pct:.2f}% missing values in '{col}' with mode: {mode_val}\")\n",
    "    \n",
    "    # Handle outliers (using a conservative approach - just flagging, not removing)\n",
    "    outlier_flags = {}\n",
    "    for col, outlier_info in quality_report['potential_outliers'].items():\n",
    "        if outlier_info['percentage'] > 0:\n",
    "            lower_bound = outlier_info['lower_bound']\n",
    "            upper_bound = outlier_info['upper_bound']\n",
    "            outlier_flags[f'{col}_outlier'] = (\n",
    "                (cleaned_df[col] < lower_bound) | (cleaned_df[col] > upper_bound)\n",
    "            )\n",
    "            print(f\"Flagged {outlier_info['count']} potential outliers in '{col}'\")\n",
    "    \n",
    "    # Add outlier flags to the dataframe\n",
    "    if outlier_flags:\n",
    "        for col, flag in outlier_flags.items():\n",
    "            cleaned_df[col] = flag\n",
    "    \n",
    "    # Convert data types where appropriate\n",
    "    for col in cleaned_df.columns:\n",
    "        # Try to convert object columns to datetime if they look like dates\n",
    "        if cleaned_df[col].dtype == 'object':\n",
    "            try:\n",
    "                # Check if this might be a date column\n",
    "                if cleaned_df[col].str.contains('-|/|:').any():\n",
    "                    cleaned_df[col] = pd.to_datetime(cleaned_df[col], errors='ignore')\n",
    "                    if cleaned_df[col].dtype.kind == 'M':  # If conversion was successful\n",
    "                        print(f\"Converted '{col}' to datetime.\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Try to convert string numeric columns to float\n",
    "        if cleaned_df[col].dtype == 'object':\n",
    "            try:\n",
    "                # Remove currency symbols and commas if present\n",
    "                temp_series = cleaned_df[col].str.replace('[$,]', '', regex=True)\n",
    "                # Check if all non-NA values can be converted to float\n",
    "                if pd.to_numeric(temp_series, errors='coerce').notna().all():\n",
    "                    cleaned_df[col] = pd.to_numeric(temp_series)\n",
    "                    print(f\"Converted '{col}' to numeric.\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(\"Data cleaning completed!\")\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "def save_cleaned_dataset(df, output_path):\n",
    "    \"\"\"\n",
    "    Save the cleaned dataset to a file.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset to save.\n",
    "    output_path (str): Path to save the cleaned dataset.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if saved successfully, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if output_path.endswith('.csv'):\n",
    "            df.to_csv(output_path, index=False)\n",
    "        elif output_path.endswith(('.xls', '.xlsx')):\n",
    "            df.to_excel(output_path, index=False)\n",
    "        else:\n",
    "            # Default to CSV\n",
    "            output_path = output_path + '.csv' if not '.' in output_path else output_path.split('.')[0] + '.csv'\n",
    "            df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"Cleaned dataset saved to: {output_path}\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving cleaned dataset: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    # Use a fixed file path for the student performance data\n",
    "    file_path = \"student_performance_data.csv\"\n",
    "    print(f\"Attempting to load: {file_path}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    try:\n",
    "        # Try direct pandas loading to bypass our custom function\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Check data quality\n",
    "    quality_report = check_data_quality(df)\n",
    "    \n",
    "    # Print quality report summary\n",
    "    print(\"\\n=== Data Quality Report ===\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(f\"Duplicate rows: {quality_report['duplicates']['count']} ({quality_report['duplicates']['percentage']:.2f}%)\")\n",
    "    \n",
    "    # Count columns with missing values properly\n",
    "    columns_with_missing = sum(1 for col, count in quality_report['missing_values']['count'].items() if count > 0)\n",
    "    print(f\"Columns with missing values: {columns_with_missing}\")\n",
    "    \n",
    "    print(f\"Columns with potential outliers: {len(quality_report['potential_outliers'])}\")\n",
    "    \n",
    "    # Visualize data distributions\n",
    "    print(\"\\nGenerating distribution visualizations...\")\n",
    "    visualize_distributions(df)\n",
    "    \n",
    "    # Clean the dataset\n",
    "    cleaned_df = clean_dataset(df, quality_report)\n",
    "    \n",
    "    # Compare before and after\n",
    "    print(\"\\n=== Before vs. After Cleaning ===\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    print(f\"Cleaned shape: {cleaned_df.shape}\")\n",
    "    print(\"\\nMissing values before:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\nMissing values after:\")\n",
    "    print(cleaned_df.isnull().sum())\n",
    "    \n",
    "    # Ask whether to save the cleaned dataset\n",
    "    save_option = input(\"\\nDo you want to save the cleaned dataset? (yes/no): \").lower()\n",
    "    if save_option in ['yes', 'y']:\n",
    "        output_path = input(\"Enter the path to save the cleaned dataset: \")\n",
    "        save_cleaned_dataset(cleaned_df, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
