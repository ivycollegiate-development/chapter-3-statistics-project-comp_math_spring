{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df = pd.read_csv(\"student_performance_data.csv\")\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load a dataset and display basic information about it.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the dataset file.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The loaded dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to infer the file type from extension\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.endswith(('.xls', '.xlsx')):\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {file_path}\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\nDataset Shape:\", df.shape)\n",
    "        print(\"\\nFirst 5 Rows:\")\n",
    "        print(df.head())\n",
    "        print(\"\\nData Types:\")\n",
    "        print(df.dtypes)\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(df.describe())\n",
    "        print(\"\\nMissing Values per Column:\")\n",
    "        print(df.isnull().sum())\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(df):\n",
    "    \"\"\"\n",
    "    Check data quality issues such as missing values, duplicates, and outliers.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset to check.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing data quality metrics.\n",
    "    \"\"\"\n",
    "    quality_report = {}\n",
    "    \n",
    "    # Check missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(df)) * 100\n",
    "    quality_report['missing_values'] = {\n",
    "        'count': missing_values,\n",
    "        'percentage': missing_percentage\n",
    "    }\n",
    "    \n",
    "    # TODO: Check duplicates\n",
    "    # 1. Count the number of duplicate rows\n",
    "    # 2. Calculate the percentage of duplicate rows\n",
    "    # 3. Add this information to the quality_report dictionary\n",
    "    \n",
    "    # TODO: Check for potential outliers in numeric columns\n",
    "    # 1. For each numeric column, calculate Q1, Q3, and IQR\n",
    "    # 2. Define lower and upper bounds (Q1 - 1.5*IQR and Q3 + 1.5*IQR)\n",
    "    # 3. Count values outside these bounds\n",
    "    # 4. Add this information to the quality_report dictionary\n",
    "    \n",
    "    return quality_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(df):\n",
    "    \"\"\"\n",
    "    Check data quality issues such as missing values, duplicates, and outliers.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset to check.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing data quality metrics.\n",
    "    \"\"\"\n",
    "    quality_report = {}\n",
    "    \n",
    "    # Check missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(df)) * 100\n",
    "    quality_report['missing_values'] = {\n",
    "        'count': missing_values,\n",
    "        'percentage': missing_percentage\n",
    "    }\n",
    "    \n",
    "    # TODO: Check duplicates\n",
    "    # 1. Count the number of duplicate rows\n",
    "    # 2. Calculate the percentage of duplicate rows\n",
    "    # 3. Add this information to the quality_report dictionary\n",
    "    \n",
    "    # TODO: Check for potential outliers in numeric columns\n",
    "    # 1. For each numeric column, calculate Q1, Q3, and IQR\n",
    "    # 2. Define lower and upper bounds (Q1 - 1.5*IQR and Q3 + 1.5*IQR)\n",
    "    # 3. Count values outside these bounds\n",
    "    # 4. Add this information to the quality_report dictionary\n",
    "    \n",
    "    return quality_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df, quality_report=None):\n",
    "    \"\"\"\n",
    "    Clean the dataset based on identified issues.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset to clean.\n",
    "    quality_report (dict, optional): Data quality report generated by check_data_quality.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The cleaned dataset.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Generate quality report if not provided\n",
    "    if quality_report is None:\n",
    "        quality_report = check_data_quality(df)\n",
    "    \n",
    "    print(\"Starting data cleaning process...\")\n",
    "    \n",
    "    # TODO: Handle duplicates\n",
    "    # 1. Remove duplicate rows\n",
    "    # 2. Print the number of rows removed\n",
    "    \n",
    "    # TODO: Handle missing values\n",
    "    # 1. For each column with missing values:\n",
    "    #    a. If missing percentage is high (e.g., > 50%), consider recommending to drop the column\n",
    "    #    b. For numeric columns, impute with median\n",
    "    #    c. For categorical columns, impute with mode\n",
    "    # 2. Print the imputation details for each column\n",
    "    \n",
    "    # TODO: Handle outliers\n",
    "    # 1. Create flags for outliers in each numeric column\n",
    "    # 2. Add these flags as new columns in the dataframe\n",
    "    # 3. Print the number of outliers identified\n",
    "    \n",
    "    # TODO: Convert data types\n",
    "    # 1. Try to convert object columns to datetime if they look like dates\n",
    "    # 2. Try to convert string numeric columns to float\n",
    "    # 3. Print the successful conversions\n",
    "    \n",
    "    print(\"Data cleaning completed!\")\n",
    "    \n",
    "    return cleaned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_dataset(df, output_path):\n",
    "    \"\"\"\n",
    "    Save the cleaned dataset to a file.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset to save.\n",
    "    output_path (str): Path to save the cleaned dataset.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if saved successfully, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Save the DataFrame based on file extension\n",
    "        # 1. If output_path ends with .csv, save as CSV\n",
    "        # 2. If output_path ends with .xls or .xlsx, save as Excel\n",
    "        # 3. If output_path doesn't have a recognized extension, save as CSV with .csv appended\n",
    "        \n",
    "        print(f\"Cleaned dataset saved to: {output_path}\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving cleaned dataset: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_dataset(df, output_path):\n",
    "    \"\"\"\n",
    "    Save the cleaned dataset to a file.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The dataset to save.\n",
    "    output_path (str): Path to save the cleaned dataset.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if saved successfully, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Save the DataFrame based on file extension\n",
    "        # 1. If output_path ends with .csv, save as CSV\n",
    "        # 2. If output_path ends with .xls or .xlsx, save as Excel\n",
    "        # 3. If output_path doesn't have a recognized extension, save as CSV with .csv appended\n",
    "        \n",
    "        print(f\"Cleaned dataset saved to: {output_path}\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving cleaned dataset: {e}\")\n",
    "        return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
